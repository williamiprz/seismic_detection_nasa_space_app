{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from scipy.linalg import svd\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create tfrecord\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Convierte un valor en una lista de bytes y crea una caracter√≠stica.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "# Functions to process data\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=4):\n",
    "    nyq = 0.5 * fs \n",
    "    normal_cutoff = cutoff / nyq \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "def ssa(signal, window_length):\n",
    "    N = len(signal)\n",
    "    X = np.array([signal[i:i + window_length] for i in range(N - window_length + 1)])\n",
    "    \n",
    "    U, S, VT = svd(X, full_matrices=False)\n",
    "    \n",
    "    return U, S, VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features in a dictionary, accepts an array with shape (6, 500) = (channels, samples)\n",
    "\n",
    "def window_example(window, prediction, name):\n",
    "    feature = {\n",
    "        'reconstructed_signal': float_feature(window[0, :].flatten()),\n",
    "        'saa1': float_feature(window[1, :].flatten()),\n",
    "        'saa2': float_feature(window[2, :].flatten()),\n",
    "        'saa3': float_feature(window[3, :].flatten()),\n",
    "        'saa4': float_feature(window[4, :].flatten()),\n",
    "        'saa5': float_feature(window[5, :].flatten()),\n",
    "        'pred': float_feature(prediction.flatten()),\n",
    "        'name': bytes_feature(name.encode('utf-8')),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord(csv_dir, tfrecord_path):\n",
    "\n",
    "    csvs = sorted(glob(f\"{csv_dir}/*.csv\"))\n",
    "    with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "\n",
    "        for csv in tqdm(csvs, desc=\"Leyendo csvs\"):\n",
    "\n",
    "            file = os.path.splitext(os.path.basename(csv))[0]\n",
    "\n",
    "            # Reading csv signal\n",
    "            data_file = f'./data/lunar/training/data/S12_GradeA/{file}.csv'\n",
    "            data_cat = pd.read_csv(data_file)\n",
    "\n",
    "            # Reading Catalog\n",
    "            cat_lunar_train_dir = './data/lunar/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "            cat_lunar_train = pd.read_csv(cat_lunar_train_dir)\n",
    "\n",
    "            detection = cat_lunar_train[cat_lunar_train['filename'] == file].iloc[0]['time_rel(sec)']\n",
    "\n",
    "            # Columns \"time_rel(sec)\" y \"velocity(m/s)\"\n",
    "            csv_times = np.array(data_cat['time_rel(sec)'].tolist())\n",
    "            csv_data = np.array(data_cat['velocity(m/s)'].tolist())\n",
    "\n",
    "            # Filter parameters\n",
    "            highcut = 0.9  # Frecuencia de corte para el filtro pasa-bajo\n",
    "            sampling_rate = 1 / 0.15  # Frecuencia de muestreo, derivada de 1 muestra cada 0.15 segundos\n",
    "\n",
    "            csv_data = butter_lowpass_filter(csv_data, cutoff=highcut, fs=sampling_rate)\n",
    "\n",
    "            closest_index = np.argmin(np.abs(csv_times - detection))\n",
    "\n",
    "            # Creating wide window for data labeled with 799 samples\n",
    "            start_index = max(0, closest_index - 399)\n",
    "            end_index = min(len(csv_data), closest_index + 399)\n",
    "\n",
    "            window_data = csv_data[start_index:end_index + 1]\n",
    "\n",
    "            remaining_data = np.concatenate([csv_data[:start_index], csv_data[end_index + 1:]])\n",
    "\n",
    "            # windows parameters: for slidding window\n",
    "            window_size = 500\n",
    "            stride = 1\n",
    "\n",
    "            # Number of windows\n",
    "            n_windows = len(window_data) - window_size + 1\n",
    "\n",
    "            windows = np.zeros((n_windows, window_size))\n",
    "\n",
    "            # Generating windows\n",
    "            for i in range(n_windows):\n",
    "                windows[i] = window_data[i:i + window_size]\n",
    "            \n",
    "            detection_index = 399\n",
    "            detection_positions = np.zeros(n_windows)\n",
    "\n",
    "            # Position of detection in each window\n",
    "            for i in range(n_windows):\n",
    "                detection_positions[i] = detection_index - i\n",
    "            \n",
    "            with_6_seismic = []\n",
    "            for i in range(len(windows)):\n",
    "\n",
    "                # Compute SSA \n",
    "                window_length = 75\n",
    "                U, S, VT = ssa(windows[i], window_length)\n",
    "\n",
    "                # Reconstruction with first 20 components\n",
    "                n_components = 20 \n",
    "                reconstructed_trajectory_matrix = np.dot(U[:, :n_components], np.diag(S[:n_components])).dot(VT[:n_components, :])\n",
    "\n",
    "                reconstructed_signal = np.zeros(len(windows[0]))\n",
    "                counts = np.zeros(len(windows[0]))\n",
    "\n",
    "                for i in range(window_length):\n",
    "                    reconstructed_signal[i: i + len(reconstructed_trajectory_matrix[:, i])] += reconstructed_trajectory_matrix[:, i]\n",
    "                    counts[i: i + len(reconstructed_trajectory_matrix[:, i])] += 1\n",
    "\n",
    "                reconstructed_signal /= counts\n",
    "\n",
    "                # Getting singular values by pairs\n",
    "                valores_singulares = []\n",
    "                for i in range(0, 10, 2): \n",
    "                    avg_U = (U[:, i:i+1] + U[:, i+1:i+2]) / 2\n",
    "                    avg_S = (S[i:i+1] + S[i+1:i+2]) / 2\n",
    "                    avg_VT = (VT[i:i+1, :] + VT[i+1:i+2, :]) / 2\n",
    "\n",
    "                    reconstructed_trajectory_matrix_i = np.dot(avg_U, np.diag(avg_S)).dot(avg_VT)\n",
    "\n",
    "                    reconstructed_signal_i = np.zeros(len(windows[0]))\n",
    "                    counts_i = np.zeros(len(windows[0]))\n",
    "\n",
    "                    for j in range(window_length):\n",
    "                        reconstructed_signal_i[j: j + len(reconstructed_trajectory_matrix_i[:, j])] += reconstructed_trajectory_matrix_i[:, j]\n",
    "                        counts_i[j: j + len(reconstructed_trajectory_matrix_i[:, j])] += 1\n",
    "\n",
    "                    reconstructed_signal_i /= counts_i\n",
    "\n",
    "                    valores_singulares.append(reconstructed_signal_i)\n",
    "\n",
    "                valores_singulares = np.array(valores_singulares)\n",
    "\n",
    "                # Combining reconstructed and singular values signals\n",
    "                combined_array = np.zeros((6, 500))\n",
    "\n",
    "                combined_array[0, :] = reconstructed_signal\n",
    "\n",
    "                combined_array[1:, :] = valores_singulares\n",
    "\n",
    "                with_6_seismic.append(combined_array)\n",
    "\n",
    "            with_6_seismic = np.array(with_6_seismic)\n",
    "\n",
    "            # Doing the same for data with no seismic detection\n",
    "\n",
    "            # Parameters\n",
    "            window_size = 500\n",
    "            num_windows = 500\n",
    "            remaining_data_length = len(remaining_data)\n",
    "\n",
    "            max_start_index = remaining_data_length - window_size\n",
    "\n",
    "            start_indices = np.random.choice(max_start_index, size=num_windows, replace=False)\n",
    "\n",
    "            windows_no_label = np.array([remaining_data[start:start + window_size] for start in start_indices])\n",
    "            ###############################\n",
    "            no_detection = np.full(500, -1)\n",
    "            ###############################\n",
    "\n",
    "            with_6_noise = []\n",
    "            for i in range(len(windows_no_label)):\n",
    "\n",
    "                window_length = 75\n",
    "                U, S, VT = ssa(windows_no_label[i], window_length)\n",
    "\n",
    "                n_components = 20\n",
    "                reconstructed_trajectory_matrix = np.dot(U[:, :n_components], np.diag(S[:n_components])).dot(VT[:n_components, :])\n",
    "\n",
    "                reconstructed_signal = np.zeros(len(windows_no_label[0]))\n",
    "                counts = np.zeros(len(windows_no_label[0]))\n",
    "\n",
    "                for i in range(window_length):\n",
    "                    reconstructed_signal[i: i + len(reconstructed_trajectory_matrix[:, i])] += reconstructed_trajectory_matrix[:, i]\n",
    "                    counts[i: i + len(reconstructed_trajectory_matrix[:, i])] += 1\n",
    "\n",
    "                reconstructed_signal /= counts\n",
    "\n",
    "                valores_singulares = []\n",
    "                for i in range(0, 10, 2):\n",
    "                    avg_U = (U[:, i:i+1] + U[:, i+1:i+2]) / 2\n",
    "                    avg_S = (S[i:i+1] + S[i+1:i+2]) / 2\n",
    "                    avg_VT = (VT[i:i+1, :] + VT[i+1:i+2, :]) / 2\n",
    "\n",
    "                    reconstructed_trajectory_matrix_i = np.dot(avg_U, np.diag(avg_S)).dot(avg_VT)\n",
    "\n",
    "                    reconstructed_signal_i = np.zeros(len(windows_no_label[0]))\n",
    "                    counts_i = np.zeros(len(windows_no_label[0]))\n",
    "\n",
    "                    for j in range(window_length):\n",
    "                        reconstructed_signal_i[j: j + len(reconstructed_trajectory_matrix_i[:, j])] += reconstructed_trajectory_matrix_i[:, j]\n",
    "                        counts_i[j: j + len(reconstructed_trajectory_matrix_i[:, j])] += 1\n",
    "\n",
    "                    reconstructed_signal_i /= counts_i\n",
    "\n",
    "                    valores_singulares.append(reconstructed_signal_i)\n",
    "\n",
    "                valores_singulares = np.array(valores_singulares)\n",
    "\n",
    "                combined_array = np.zeros((6, 500))\n",
    "\n",
    "                combined_array[0, :] = reconstructed_signal\n",
    "\n",
    "                combined_array[1:, :] = valores_singulares\n",
    "\n",
    "                with_6_noise.append(combined_array)\n",
    "\n",
    "            with_6_noise = np.array(with_6_noise)\n",
    "\n",
    "            # Concatenign data labeled an no labeled\n",
    "            \n",
    "            combinated_6_signals = np.concatenate([with_6_seismic, with_6_noise], axis=0) \n",
    "            combinated_preds = np.concatenate([detection_positions, no_detection] , axis=0)\n",
    "\n",
    "            # Normalizing based in exploring_data.ipyb results\n",
    "\n",
    "            min_val = -2e-8\n",
    "            max_val = 2e-8\n",
    "\n",
    "            # Normalizaci√≥n between -1 y 1\n",
    "            combinated_6_signals_normalized = 2 * (combinated_6_signals - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "            combinated_preds_normalized = np.where(combinated_preds == -1, -1, combinated_preds / 499)\n",
    "\n",
    "            # To .tfrecord file\n",
    "            for i in range(len(combinated_6_signals_normalized)):\n",
    "                example = window_example(combinated_6_signals_normalized[i], combinated_preds_normalized[i], file)\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leyendo csvs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [21:05<00:00, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dir = './data/lunar/training/data/S12_GradeA/'\n",
    "    tfrecord_train = './data/lunar/train_ds.tfrecords'\n",
    "\n",
    "    create_tfrecord(train_dir, tfrecord_train)\n",
    "    print(\"====================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
